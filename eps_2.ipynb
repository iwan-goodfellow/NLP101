{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be9b78e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2ebf38",
   "metadata": {},
   "source": [
    "# Sebuah Sejarah üîñ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06adec39",
   "metadata": {},
   "source": [
    "## Conventional ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e21a885",
   "metadata": {},
   "source": [
    "Jadi awal banget itu kita solve masalah kek text misal classification dll itu approachnya adalah dengan menggunakan ml konvensional, jadi `awalnya teks akan dicleaning` setelah itu di `extract` umumnya sih pake `BoW atau Tf-Idf` or dicombine. After that langsung kita `cemplungin ke model`. Modelnya apa? ini terserah aja sih."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d7c748",
   "metadata": {},
   "source": [
    "### BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b842f8b",
   "metadata": {},
   "source": [
    "Mungkin aga lompat, kemarin kita belum sempet bahas tentang BoW dan Tf-Idf di Episode_1. <br>\n",
    "Jadi apa it BoW? Bag-of-Words adalah metode sederhana untuk representasi teks.  \n",
    "Dia menghitung **berapa kali setiap kata muncul** dalam sebuah dokumen, tanpa peduli urutan atau struktur kalimat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5e9d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"saya makan bakso\",\n",
    "    \"bakso enak dan panas\",\n",
    "    \"saya juga suka makan sate\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7621ad68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW DataFrame:\n",
      "   bakso  dan  enak  juga  makan  panas  sate  saya  suka\n",
      "0      1    0     0     0      1      0     0     1     0\n",
      "1      1    1     1     0      0      1     0     0     0\n",
      "2      0    0     0     1      1      0     1     1     1\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "# Buat DataFrame\n",
    "df_bow = pd.DataFrame(bow_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "print(\"BoW DataFrame:\")\n",
    "print(df_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8399609",
   "metadata": {},
   "source": [
    "Tuh keliatan kan temen2? jadi si bakso keluar 2x, terus enak 1x dll. Jadi bener2 cuma itung aja gitu berapa kali dia muncul?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a123a5",
   "metadata": {},
   "source": [
    "### Tf-Idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2bcc01",
   "metadata": {},
   "source": [
    "Lanjut, TF-Idf itu apa? jadi Term Frequency - Inverse document frequency itu ngasi bobot berdasarkan sbrpa penting sbuah kata dalam suatu dokumen relatif thd seluruh koleksi dokumen. **Jadi kl ada kata yg sering muncul di dok tertentu, tapi jarang di dok lain, nah itu penting!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33155d7e",
   "metadata": {},
   "source": [
    "Masih bingung kenapa gitu? okee tak kasih short example. Kalo kita lagi baca berita di koran.\n",
    "- Berita 1\n",
    "‚ÄúTimnas Indonesia berhasil menang 3-0 dalam laga semifinal melawan Senegal Piala Dunia. Hatrick berhasil disarangkan oleh Ridwan. Pelatih memujinya, kita telah menemukan kombinasi kehebatan Ronaldo dan Messi dalam satu pemuda bernama Ridwan‚Äù\n",
    "- Berita 2\n",
    "‚ÄúPresiden meresmikan pembangunan jembatan di Kalimantan. Menteri PUPR turut hadir dalam peresmian.‚Äù\n",
    "- Berita 3\n",
    "‚ÄúBank Indonesia menurunkan suku bunga acuan. Inflasi diperkirakan tidak terkendali hingga akhir tahun.‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaf295f",
   "metadata": {},
   "source": [
    "Nah disitu kita ambil 2 kata deh, `Indonesia` dan `Ridwan`. Indonesia muncul di seluruh dokumen berita, jadi bukan kata unik, biasanya kita classify dia juga jadi \"stopwords\". Nah Ridwan cuma muncul di 1 dokumen yaitu berita olahraga, dia disebut di konteks sepakbola hingga 2 kali wicis sering, tapi ga muncul di berita ekonomi dan politik maka Tf-Idf dia tinggi karena unik dan penting di konteks artikel tsb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4037a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "berita = [\n",
    "    \"Timnas Indonesia berhasil menang 3-0 dalam laga semifinal melawan Senegal Piala Dunia. Hatrick berhasil disarangkan oleh Ridwan. Pelatih memujinya, kita telah menemukan kombinasi kehebatan Ronaldo dan Messi dalam satu pemuda bernama Ridwan\",\n",
    "    \"Presiden meresmikan pembangunan jembatan di Kalimantan. Menteri PUPR turut hadir dalam peresmian\",\n",
    "    \"Bank Indonesia menurunkan suku bunga acuan. Inflasi diperkirakan tidak terkendali hingga akhir tahun\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d497c6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF DataFrame:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "messi",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "oleh",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pelatih",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pembangunan",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pemuda",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "peresmian",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "piala",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "presiden",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pupr",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ridwan",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ronaldo",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "satu",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "semifinal",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "senegal",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "suku",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tahun",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "telah",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "terkendali",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tidak",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "timnas",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "turut",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "80b75b8c-ac50-4203-b4c9-515a4e3f3576",
       "rows": [
        [
         "0",
         "0.16929225311038873",
         "0.16929225311038873",
         "0.16929225311038873",
         "0.0",
         "0.16929225311038873",
         "0.0",
         "0.16929225311038873",
         "0.0",
         "0.0",
         "0.33858450622077746",
         "0.16929225311038873",
         "0.16929225311038873",
         "0.16929225311038873",
         "0.16929225311038873",
         "0.0",
         "0.0",
         "0.16929225311038873",
         "0.0",
         "0.0",
         "0.16929225311038873",
         "0.0"
        ],
        [
         "1",
         "0.0",
         "0.0",
         "0.0",
         "0.2938838601653297",
         "0.0",
         "0.2938838601653297",
         "0.0",
         "0.2938838601653297",
         "0.2938838601653297",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.2938838601653297"
        ],
        [
         "2",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.2819598745697001",
         "0.2819598745697001",
         "0.0",
         "0.2819598745697001",
         "0.2819598745697001",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 21,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messi</th>\n",
       "      <th>oleh</th>\n",
       "      <th>pelatih</th>\n",
       "      <th>pembangunan</th>\n",
       "      <th>pemuda</th>\n",
       "      <th>peresmian</th>\n",
       "      <th>piala</th>\n",
       "      <th>presiden</th>\n",
       "      <th>pupr</th>\n",
       "      <th>ridwan</th>\n",
       "      <th>...</th>\n",
       "      <th>satu</th>\n",
       "      <th>semifinal</th>\n",
       "      <th>senegal</th>\n",
       "      <th>suku</th>\n",
       "      <th>tahun</th>\n",
       "      <th>telah</th>\n",
       "      <th>terkendali</th>\n",
       "      <th>tidak</th>\n",
       "      <th>timnas</th>\n",
       "      <th>turut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.169292</td>\n",
       "      <td>0.169292</td>\n",
       "      <td>0.169292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169292</td>\n",
       "      <td>0.169292</td>\n",
       "      <td>0.169292</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.169292</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.169292</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293884</td>\n",
       "      <td>0.293884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.28196</td>\n",
       "      <td>0.28196</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.28196</td>\n",
       "      <td>0.28196</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      messi      oleh   pelatih  pembangunan    pemuda  peresmian     piala  \\\n",
       "0  0.169292  0.169292  0.169292     0.000000  0.169292   0.000000  0.169292   \n",
       "1  0.000000  0.000000  0.000000     0.293884  0.000000   0.293884  0.000000   \n",
       "2  0.000000  0.000000  0.000000     0.000000  0.000000   0.000000  0.000000   \n",
       "\n",
       "   presiden      pupr    ridwan  ...      satu  semifinal   senegal     suku  \\\n",
       "0  0.000000  0.000000  0.338585  ...  0.169292   0.169292  0.169292  0.00000   \n",
       "1  0.293884  0.293884  0.000000  ...  0.000000   0.000000  0.000000  0.00000   \n",
       "2  0.000000  0.000000  0.000000  ...  0.000000   0.000000  0.000000  0.28196   \n",
       "\n",
       "     tahun     telah  terkendali    tidak    timnas     turut  \n",
       "0  0.00000  0.169292     0.00000  0.00000  0.169292  0.000000  \n",
       "1  0.00000  0.000000     0.00000  0.00000  0.000000  0.293884  \n",
       "2  0.28196  0.000000     0.28196  0.28196  0.000000  0.000000  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(berita)\n",
    "\n",
    "# Dapatkan nama fitur (kata-kata)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Buat DataFrame\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "print(\"TF-IDF DataFrame:\")\n",
    "df_tfidf.iloc[:,30:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e53619",
   "metadata": {},
   "source": [
    "Nah kalo kalian aware disitu keliatan vektor Ridwan itu tinggi banget dibanding yg lainnya. Itu deh salah satu contohnya. Semoga makin clear yaa..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ebb874",
   "metadata": {},
   "source": [
    "Karena disini kita cuma bakal bahas sejarahnya, so gaakan ada coding2 yg belibet jadi mostly aku bakal cuma kasi flownya aja dan big ideanya aja. Lesgo!!! Oiya tadi flownya udah yaa. Kita masuk trivia aja haha...\n",
    "> Berarti pendekatan konvensional ini pasti kalah dibanding RNN, Transformers dan sebangsanya? jawaban singkatnya **ga jg**, its depend on ur case. Kalo emang datanya ga yg rumit2 secara semantik dan emang butuh ngejar waktu its good to take the approach. Sedikit cerita aku pernah make pendekatan ini di Final GEMASTIK Data Mining(salah satu lomba impian anak2 it indo hehe), pas itu di leaderboard aku peringkat-7 which is not bad, aku inget banget yg juara 1 iya dia pake transformers, tapi yg dibawah2ku (context:final gemastik itu 20 tim) peringkat 8 dst ada beberapa pake transformers tapi problemnya adalah waktu buat training even fine tunenya pun lama bget ga kekejar di waktu yg singkat. So inget lagi kalian mo ngapain? tentuin goals dlu baru choose ur best weapon ‚öîÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e91872",
   "metadata": {},
   "source": [
    "## RNN (Reccurent Neur Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a7645d",
   "metadata": {},
   "source": [
    "Perkembangan berikutnya yaitu RNN jadi seperti namanya \"reccurent\" berati ini adalah neural network yg diulang2. Ilustrasi gambarnya itu seperti dibawah ini."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdd558b",
   "metadata": {},
   "source": [
    "<img src=\"assets_local\\architecture rnn.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ff0e03",
   "metadata": {},
   "source": [
    "Oke mungkin masih kurang jelas ya temen2, kita coba bakal ngebreakdown lebih lanjut utk arsitektur RNN ini. Jadi aku bakal coba gambarin dan ngasih keterangan juga semoga dapat mudah dipahami."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8996c583",
   "metadata": {},
   "source": [
    "<img src=\"assets_local\\rnn.jpeg\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9db7135",
   "metadata": {},
   "source": [
    "Nah kenapa siih RNN bisa lebih bagus dari pendekatan sebelumnya kek yg kita udah apply dengan ml konvensional? \n",
    "1. RNN itu bisa tau informasi dari timestep sebelumnya jadi dia seolah2 punya memori ya though jangka pendek.\n",
    "2. Model2 konvensional kek yg udah kita tw misal naive bayes terus logreg, itu ngeassume bahwa satu titik data/fitur itu adalah independen dari other titik. Jadi kek yaudah entitas2 kata itu tadi yg seharusnya adalah dependen dipisah sama dia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f72f88",
   "metadata": {},
   "source": [
    "## LSTM (Long Short Term Memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff17332",
   "metadata": {},
   "source": [
    "Nah kita tadi udah belajar masalah RNN, apakah RNN se-OP itu? gaaa, dia juga masih punya banyak kekurangan. Coba deh kalian bayangin punya kalimat yang puanjannggg banget, nah RNN itu bakal sulit buat ngeidentify depedensi jangka panjang spt itu, jadi sequence di timestep awal itu bisa ilang ato bahasa kerennya **Vanishing Gradient** nah bisa juga sebaliknya jadi dominan banget namanya **Exploding Gradient**. Oiya karena ini reccurent which is ngulang2 dia luamaa banget..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93a5bd9",
   "metadata": {},
   "source": [
    "LSTM jadi jawaban dari problem RNN (wlupun bukan jawaban terbaik hahaha üòÅ), so LSTM pake pendekatan yang lain dimana model ini seolah-olah punya skill milih\n",
    "- apa aja sii yg penting untk diinget?\n",
    "- apa aja yg dilupain di masa lalu?<br>\n",
    "Jadi LSTM nambahin suatu **gates** yg ada 3 yaitu:\n",
    "1. Input Gate \n",
    "2. Forget Gate\n",
    "3. Output Gate<br>\n",
    "Kita bakal jelasin lebih deep di ilustrasi yg udah aku buat dibawah ini. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce79465e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d633071",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_iwan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
