{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaaca945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\backe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "073e1ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kalimat = [\n",
    "    \"I love Indonesia.\",\n",
    "    \"Sate is scrumptious.\",\n",
    "    \"Sei Caduto in un Errore Logico.\",\n",
    "    \"Sugeng rawuh! sedulur sak kabehe.\",\n",
    "    \"Punten aa, kumaha ieu teh?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cfa80e",
   "metadata": {},
   "source": [
    "# Tokenizationâ›“ï¸â€ðŸ’¥\n",
    "**How to mecah texts to unit terkecilnya bisa kata, subkata or character.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad64b5a",
   "metadata": {},
   "source": [
    "## Tradisional Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e50eaa0",
   "metadata": {},
   "source": [
    "Pendekatan pertama yg bisa dipake itu adalah as simple kita pecah2 dia berdasarkan spasi, tapi ingat ga smua kalimat yg kita pisah utk jadi kata in general bisa kita pisah spt itu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdf8957e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalimat: I love Indonesia.\n",
      "Kata: ['I', 'love', 'Indonesia.']\n",
      "Kalimat: Sate is scrumptious.\n",
      "Kata: ['Sate', 'is', 'scrumptious.']\n",
      "Kalimat: Sei Caduto in un Errore Logico.\n",
      "Kata: ['Sei', 'Caduto', 'in', 'un', 'Errore', 'Logico.']\n",
      "Kalimat: Sugeng rawuh! sedulur sak kabehe.\n",
      "Kata: ['Sugeng', 'rawuh!', 'sedulur', 'sak', 'kabehe.']\n",
      "Kalimat: Punten aa, kumaha ieu teh?\n",
      "Kata: ['Punten', 'aa,', 'kumaha', 'ieu', 'teh?']\n"
     ]
    }
   ],
   "source": [
    "for kata in kalimat:\n",
    "    kata_kata = kata.split()\n",
    "    print(f'Kalimat: {kata}')\n",
    "    print(f'Kata: {kata_kata}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8033dc",
   "metadata": {},
   "source": [
    "Nah kek gitu kalo dia pakenya split, simple sebenernya disini konsep dia itu adlah misahinnya based delimiternya(default->spasi). Tapi bisa kita liat disini minesnya dia bner2 gabisa handle tanda baca dg bagus, liat si titik selalu ngikut di kata terakhir and etc. Makanya ada teknik yg lebih bagus yaitu pake Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f195b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalimat: I love Indonesia.\n",
      "Kata: ['I', 'love', 'Indonesia', '.']\n",
      "Kalimat: Sate is scrumptious.\n",
      "Kata: ['Sate', 'is', 'scrumptious', '.']\n",
      "Kalimat: Sei Caduto in un Errore Logico.\n",
      "Kata: ['Sei', 'Caduto', 'in', 'un', 'Errore', 'Logico', '.']\n",
      "Kalimat: Sugeng rawuh! sedulur sak kabehe.\n",
      "Kata: ['Sugeng', 'rawuh', '!', 'sedulur', 'sak', 'kabehe', '.']\n",
      "Kalimat: Punten aa, kumaha ieu teh?\n",
      "Kata: ['Punten', 'aa', ',', 'kumaha', 'ieu', 'teh', '?']\n"
     ]
    }
   ],
   "source": [
    "for kata in kalimat:\n",
    "    kata_kata = word_tokenize(kata)\n",
    "    print(f'Kalimat: {kata}')\n",
    "    print(f'Kata: {kata_kata}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525f63c8",
   "metadata": {},
   "source": [
    "Keliatan kan disini bedanya, si word tokenizer itu udah misah juga sekalian sama tanda bacanya. Tapi disini problem karena secara semantik suatu kata itu ga hanya dipisah dengan \"suatu tanda baca\" atau unit dasarnya. Misal nih kek apple's itu nanti dia jadinya apple,',s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d09c8c",
   "metadata": {},
   "source": [
    "Satu lagi haha, si word tokenizer ini juga \"hancur\" ketika kita punya bahasa baru/gaul gt2lah. Misal -> `Lu kerja dimans sii? kok keliatanya gabut btz.` Nah di kamus word tokenizer itu kata2 spt dimans, gabut, btz gaada jadi dia bisa anggap sbg OOV dan nanti dia jadiin itu UNK token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e8dde5",
   "metadata": {},
   "source": [
    "## BPE in Action "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba47afb5",
   "metadata": {},
   "source": [
    "Nah model modern sekarang itu mostly pakenya Byte Pair Encoding, jadi basisnya itu udah subword ngetokenizenya. Misal Mahasiswa -> 'Maha', '##siswa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24a32fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371d3465c7db4deda71632a4a45c015d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\backe\\miniconda3\\envs\\dl_iwan\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\backe\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac07196ac6794f12918ffc361311b57b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b985c7b885a749adb59c11683662f145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2b85d4db664b6cb51fca53264391d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8063f9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalimat:I love Indonesia.\n",
      "Kata: ['[CLS]', 'i', 'love', 'indonesia', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Kalimat:Sate is scrumptious.\n",
      "Kata: ['[CLS]', 'sat', '##e', 'is', 'sc', '##rum', '##pt', '##ious', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Kalimat:Sei Caduto in un Errore Logico.\n",
      "Kata: ['[CLS]', 'se', '##i', 'cad', '##uto', 'in', 'un', 'error', '##e', 'logic', '##o', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Kalimat:Sugeng rawuh! sedulur sak kabehe.\n",
      "Kata: ['[CLS]', 'su', '##gen', '##g', 'raw', '##uh', '!', 'se', '##du', '##lu', '##r', 'sa', '##k', 'ka', '##be', '##he', '.', '[SEP]']\n",
      "Kalimat:Punten aa, kumaha ieu teh?\n",
      "Kata: ['[CLS]', 'punt', '##en', 'aa', ',', 'ku', '##mah', '##a', 'ie', '##u', 'te', '##h', '?', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer(kalimat, padding=True, \n",
    "                    truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "for i, teks in enumerate(kalimat):\n",
    "    # ambil input ids utk kalimat ke-i\n",
    "    input_ids_kalimat = encoded['input_ids'][i].tolist()\n",
    "    # convert id token jadi string token lagi\n",
    "    token_kalimat = tokenizer.convert_ids_to_tokens(input_ids_kalimat)\n",
    "    print(f'Kalimat:{teks}')\n",
    "    print(f'Kata: {token_kalimat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2630a3d3",
   "metadata": {},
   "source": [
    "Oke, mantap yaa disini keliatan kalo dia treatmentnya itu dipecah2 subwordnya. Sedikit info utk token\n",
    "- [CLS] -> sbg representasi klo ini tu kalimat\n",
    "- [PAD] -> sbg 'pengisi' biar urutan yg pendek itu punya pjg yg sama dg urutan terpanjang dlm suatu batch\n",
    "- [SEP] -> sbg penanda atau akhir dri suatu separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3532b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_iwan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
