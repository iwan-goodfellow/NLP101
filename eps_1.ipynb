{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaaca945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\backe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "073e1ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kalimat = [\n",
    "    \"I love Indonesia.\",\n",
    "    \"Sate is scrumptious.\",\n",
    "    \"Sei Caduto in un Errore Logico.\",\n",
    "    \"Sugeng rawuh! sedulur sak kabehe.\",\n",
    "    \"Punten aa, kumaha ieu teh?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cfa80e",
   "metadata": {},
   "source": [
    "# Tokenizationâ›“ï¸â€ðŸ’¥\n",
    "**How to mecah texts to unit terkecilnya bisa kata, subkata or character.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad64b5a",
   "metadata": {},
   "source": [
    "## Tradisional Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e50eaa0",
   "metadata": {},
   "source": [
    "Pendekatan pertama yg bisa dipake itu adalah as simple kita pecah2 dia berdasarkan spasi, tapi ingat ga smua kalimat yg kita pisah utk jadi kata in general bisa kita pisah spt itu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdf8957e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalimat: I love Indonesia.\n",
      "Kata: ['I', 'love', 'Indonesia.']\n",
      "Kalimat: Sate is scrumptious.\n",
      "Kata: ['Sate', 'is', 'scrumptious.']\n",
      "Kalimat: Sei Caduto in un Errore Logico.\n",
      "Kata: ['Sei', 'Caduto', 'in', 'un', 'Errore', 'Logico.']\n",
      "Kalimat: Sugeng rawuh! sedulur sak kabehe.\n",
      "Kata: ['Sugeng', 'rawuh!', 'sedulur', 'sak', 'kabehe.']\n",
      "Kalimat: Punten aa, kumaha ieu teh?\n",
      "Kata: ['Punten', 'aa,', 'kumaha', 'ieu', 'teh?']\n"
     ]
    }
   ],
   "source": [
    "for kata in kalimat:\n",
    "    kata_kata = kata.split()\n",
    "    print(f'Kalimat: {kata}')\n",
    "    print(f'Kata: {kata_kata}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8033dc",
   "metadata": {},
   "source": [
    "Nah kek gitu kalo dia pakenya split, simple sebenernya disini konsep dia itu adlah misahinnya based delimiternya(default->spasi). Tapi bisa kita liat disini minesnya dia bner2 gabisa handle tanda baca dg bagus, liat si titik selalu ngikut di kata terakhir and etc. Makanya ada teknik yg lebih bagus yaitu pake Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f195b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalimat: I love Indonesia.\n",
      "Kata: ['I', 'love', 'Indonesia', '.']\n",
      "Kalimat: Sate is scrumptious.\n",
      "Kata: ['Sate', 'is', 'scrumptious', '.']\n",
      "Kalimat: Sei Caduto in un Errore Logico.\n",
      "Kata: ['Sei', 'Caduto', 'in', 'un', 'Errore', 'Logico', '.']\n",
      "Kalimat: Sugeng rawuh! sedulur sak kabehe.\n",
      "Kata: ['Sugeng', 'rawuh', '!', 'sedulur', 'sak', 'kabehe', '.']\n",
      "Kalimat: Punten aa, kumaha ieu teh?\n",
      "Kata: ['Punten', 'aa', ',', 'kumaha', 'ieu', 'teh', '?']\n"
     ]
    }
   ],
   "source": [
    "for kata in kalimat:\n",
    "    kata_kata = word_tokenize(kata)\n",
    "    print(f'Kalimat: {kata}')\n",
    "    print(f'Kata: {kata_kata}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525f63c8",
   "metadata": {},
   "source": [
    "Keliatan kan disini bedanya, si word tokenizer itu udah misah juga sekalian sama tanda bacanya. Tapi disini problem karena secara semantik suatu kata itu ga hanya dipisah dengan \"suatu tanda baca\" atau unit dasarnya. Misal nih kek apple's itu nanti dia jadinya apple,',s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d09c8c",
   "metadata": {},
   "source": [
    "Satu lagi haha, si word tokenizer ini juga \"hancur\" ketika kita punya bahasa baru/gaul gt2lah. Misal -> `Lu kerja dimans sii? kok keliatanya gabut btz.` Nah di kamus word tokenizer itu kata2 spt dimans, gabut, btz gaada jadi dia bisa anggap sbg OOV dan nanti dia jadiin itu UNK token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e8dde5",
   "metadata": {},
   "source": [
    "## BPE in Action "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba47afb5",
   "metadata": {},
   "source": [
    "Nah model modern sekarang itu mostly pakenya Byte Pair Encoding, jadi basisnya itu udah subword ngetokenizenya. Misal Mahasiswa -> 'Maha', '##siswa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24a32fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371d3465c7db4deda71632a4a45c015d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\backe\\miniconda3\\envs\\dl_iwan\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\backe\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac07196ac6794f12918ffc361311b57b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b985c7b885a749adb59c11683662f145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2b85d4db664b6cb51fca53264391d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8063f9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalimat:I love Indonesia.\n",
      "Kata: ['[CLS]', 'i', 'love', 'indonesia', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Kalimat:Sate is scrumptious.\n",
      "Kata: ['[CLS]', 'sat', '##e', 'is', 'sc', '##rum', '##pt', '##ious', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Kalimat:Sei Caduto in un Errore Logico.\n",
      "Kata: ['[CLS]', 'se', '##i', 'cad', '##uto', 'in', 'un', 'error', '##e', 'logic', '##o', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Kalimat:Sugeng rawuh! sedulur sak kabehe.\n",
      "Kata: ['[CLS]', 'su', '##gen', '##g', 'raw', '##uh', '!', 'se', '##du', '##lu', '##r', 'sa', '##k', 'ka', '##be', '##he', '.', '[SEP]']\n",
      "Kalimat:Punten aa, kumaha ieu teh?\n",
      "Kata: ['[CLS]', 'punt', '##en', 'aa', ',', 'ku', '##mah', '##a', 'ie', '##u', 'te', '##h', '?', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer(kalimat, padding=True, \n",
    "                    truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "for i, teks in enumerate(kalimat):\n",
    "    # ambil input ids utk kalimat ke-i\n",
    "    input_ids_kalimat = encoded['input_ids'][i].tolist()\n",
    "    # convert id token jadi string token lagi\n",
    "    token_kalimat = tokenizer.convert_ids_to_tokens(input_ids_kalimat)\n",
    "    print(f'Kalimat:{teks}')\n",
    "    print(f'Kata: {token_kalimat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2630a3d3",
   "metadata": {},
   "source": [
    "Oke, mantap yaa disini keliatan kalo dia treatmentnya itu dipecah2 subwordnya. Sedikit info utk token\n",
    "- [CLS] -> sbg representasi klo ini tu kalimat\n",
    "- [PAD] -> sbg 'pengisi' biar urutan yg pendek itu punya pjg yg sama dg urutan terpanjang dlm suatu batch\n",
    "- [SEP] -> sbg penanda atau akhir dri suatu separator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfc5874",
   "metadata": {},
   "source": [
    "# Embedding ðŸ§®\n",
    "**Gimana token direpresent kedalam suatu vektor berdimensi tetap**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2766da4",
   "metadata": {},
   "source": [
    "pertama jelas kita semua udah kenal sama yg nmanya OHE ini approach basic banget dimana dia nunjukin \"is there exist or not?\" tapi ingat dia ga nunjukin suatu relasi satu dengan yg lain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a3532b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['aku',\n",
       "  'sungguh',\n",
       "  'sangat',\n",
       "  'mencintai',\n",
       "  'bakso',\n",
       "  'sapi',\n",
       "  'yang',\n",
       "  'dimasak',\n",
       "  'oleh',\n",
       "  'ibuku']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Aku sungguh sangat mencintai bakso sapi yang dimasak oleh ibuku\"\n",
    "# tokenizednya cukup kita split kali ini\n",
    "word_tokenized = [sentence.lower().split()]\n",
    "word_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e218f75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŸ¥ One-Hot Vocabulary: ['aku' 'bakso' 'dimasak' 'ibuku' 'mencintai' 'oleh' 'sangat' 'sapi'\n",
      " 'sungguh' 'yang']\n",
      "One-hot for 'bakso': 1\n"
     ]
    }
   ],
   "source": [
    "# kita coba ohe ya skrang\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "one_hot = mlb.fit_transform(word_tokenized)\n",
    "print(\"\\nðŸŸ¥ One-Hot Vocabulary:\", mlb.classes_)\n",
    "print(\"One-hot for 'bakso':\", one_hot[0][mlb.classes_.tolist().index('bakso')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60947523",
   "metadata": {},
   "source": [
    "terus yg lebi baru ada yg namanya->Word2Vec (CBOW/skip-gram), embeddingnya statis jadi vektornya tetep dan bisa divisualize juga, tapi ya itu dia still gapaham konteks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78e7a02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŸ¦ Word2Vec vector for 'bakso':\n",
      " [-0.08157917  0.04495798 -0.04137076  0.00824536  0.08498619 -0.04462177\n",
      "  0.045175   -0.0678696  -0.03548489  0.09398508]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model = Word2Vec(sentences=word_tokenized, vector_size=10, window=2, min_count=1, workers=1)\n",
    "print(\"\\nðŸŸ¦ Word2Vec vector for 'bakso':\\n\", w2v_model.wv['bakso'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f6b7588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0â†ªï¸-0.08\n",
      "1â†ªï¸0.04\n",
      "2â†ªï¸-0.04\n",
      "3â†ªï¸0.01\n",
      "4â†ªï¸0.08\n",
      "5â†ªï¸-0.04\n",
      "6â†ªï¸0.05\n",
      "7â†ªï¸-0.07\n",
      "8â†ªï¸-0.04\n",
      "9â†ªï¸0.09\n"
     ]
    }
   ],
   "source": [
    "for idx,val in enumerate(w2v_model.wv['bakso']):\n",
    "    print(f'{idx}â†ªï¸{val:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9f00afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0â†ªï¸-0.10\n",
      "1â†ªï¸0.05\n",
      "2â†ªï¸-0.09\n",
      "3â†ªï¸-0.04\n",
      "4â†ªï¸-0.00\n",
      "5â†ªï¸-0.00\n",
      "6â†ªï¸-0.08\n",
      "7â†ªï¸0.10\n",
      "8â†ªï¸0.05\n",
      "9â†ªï¸0.09\n"
     ]
    }
   ],
   "source": [
    "for idx,val in enumerate(w2v_model.wv['sapi']):\n",
    "    print(f'{idx}â†ªï¸{val:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b439c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0â†ªï¸-0.01\n",
      "1â†ªï¸0.00\n",
      "2â†ªï¸0.05\n",
      "3â†ªï¸0.09\n",
      "4â†ªï¸-0.09\n",
      "5â†ªï¸-0.07\n",
      "6â†ªï¸0.06\n",
      "7â†ªï¸0.09\n",
      "8â†ªï¸-0.05\n",
      "9â†ªï¸-0.04\n"
     ]
    }
   ],
   "source": [
    "for idx,val in enumerate(w2v_model.wv['ibuku']):\n",
    "    print(f'{idx}â†ªï¸{val:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf512f2",
   "metadata": {},
   "source": [
    "sebenernya secara \"kasarnya\" kita udah bisa liat kalo emang si bakso itu lebih deket dengan sapi, tapi kita akan validasi once again pke cosine similarity(lebih ilmiah jg wkwk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79373d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fea11546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil vektor dari model\n",
    "v_ibuku = w2v_model.wv['ibuku']\n",
    "v_sapi = w2v_model.wv['sapi']\n",
    "v_bakso = w2v_model.wv['bakso']\n",
    "\n",
    "# Bentuk ke array 2D biar bisa diproses sklearn\n",
    "vectors = np.array([v_ibuku, v_sapi, v_bakso])\n",
    "labels = ['ibuku', 'sapi', 'bakso']\n",
    "\n",
    "# Hitung cosine similarity antar semua pasangan\n",
    "sim_matrix = cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abe8bc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¥ Cosine Similarity Matrix\n",
      "ibuku â†”ï¸ sapi = -0.22\n",
      "ibuku â†”ï¸ bakso = -0.27\n",
      "sapi â†”ï¸ ibuku = -0.22\n",
      "sapi â†”ï¸ bakso = 0.25\n",
      "bakso â†”ï¸ ibuku = -0.27\n",
      "bakso â†”ï¸ sapi = 0.25\n"
     ]
    }
   ],
   "source": [
    "# bandingin dia masing2\n",
    "print(\"\\nðŸ”¥ Cosine Similarity Matrix\")\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels)):\n",
    "            if i!=j:\n",
    "                print(f\"{labels[i]} â†”ï¸ {labels[j]} = {sim_matrix[i][j]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a971d4a",
   "metadata": {},
   "source": [
    "terlihat bahwa memang bakso dan sapi punya cosine sim yg tinggi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5655f84f",
   "metadata": {},
   "source": [
    "terus ada Glove, sebenernya disini utk main conceptnya hampir sama pendekatannya, hanya saja dia basis matrixnya co-occurence, dia juga pretrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762ee677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30217a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
      "\n",
      "ðŸŸ¨ GloVe vector for 'bakso':\n",
      " [ 0.033904   0.47768   -0.35009   -0.03551    0.40425   -0.13836\n",
      "  0.40631   -0.34635    0.30708   -0.20823   -0.22807    0.010479\n",
      "  0.01501    0.23469   -0.21317    0.33624   -0.07122    0.29014\n",
      "  0.39669    0.31358    0.10733    0.19041   -0.42277   -0.12968\n",
      " -0.28085    0.37466   -0.20913    0.41805   -0.02033    0.26688\n",
      "  0.035072  -0.026574   0.0015617 -0.45732    0.4532     0.22955\n",
      "  0.16361   -0.33732   -0.0030354 -0.4692     0.039717   0.31478\n",
      "  0.20568   -0.017727   0.33816   -0.31504    0.024866   0.26932\n",
      " -0.0027975  0.7625    -0.20579    0.09217   -0.24812   -0.45499\n",
      " -0.14886    0.72996   -0.12679    0.015581  -0.78166   -0.099686\n",
      "  0.57149   -0.4424    -0.45965    0.27243   -0.30197    0.14087\n",
      " -0.10295   -0.43336   -0.16778   -0.12508   -0.2493     0.29616\n",
      "  0.35317    0.66554   -0.0046357  0.030268  -0.32464    0.077964\n",
      "  0.47472   -0.17878    0.021797  -0.48949    0.28678    0.020525\n",
      "  0.39643   -0.025419  -0.31486   -0.044168   0.19627   -0.12278\n",
      " -0.14977    0.25025    0.24971   -0.50337    0.25609   -0.27769\n",
      "  0.11422    0.21818   -0.23245    0.0081307]\n"
     ]
    }
   ],
   "source": [
    "# import gensim.downloader as api\n",
    "glove= api.load(\"glove-wiki-gigaword-100\")  # 100 dimensi\n",
    "\n",
    "try:\n",
    "    print(\"\\nðŸŸ¨ GloVe vector for 'bakso':\\n\", glove['bakso'])\n",
    "except:\n",
    "    print(\"\\nðŸŸ¨ 'bakso' not found in GloVe vocab (check casing/punctuation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f7b1c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŸ¨ GloVe vector for 'sapi':\n",
      " [-0.090403   0.11407   -0.021357   0.2734     0.21941   -0.3912\n",
      " -0.41366    0.13317    0.40228   -0.82692   -0.11431   -0.087375\n",
      " -0.75536    0.015493   0.74931    0.28701    0.1784     0.091627\n",
      "  0.30042    0.048118   0.19482   -0.79823    0.091771   0.23247\n",
      " -0.1491     0.32454    0.14852    0.34033   -0.22677    0.31044\n",
      "  0.81392   -0.012929   0.18009   -0.02811    0.23227    0.0047932\n",
      " -0.07427   -0.10136    0.74419    0.015942  -0.5788    -0.48526\n",
      "  0.31832    0.20384   -0.040021  -0.53469   -0.2496     0.073077\n",
      "  0.31809    0.44468   -0.36439    0.16193    0.041966  -0.2763\n",
      "  0.32295    0.73637   -0.22975   -0.5447    -0.1591    -0.51163\n",
      " -0.10965    0.12902   -0.23842    0.015497  -0.67473   -0.57197\n",
      " -0.36842   -0.034412  -0.098011  -0.1846     0.11332    0.57416\n",
      "  0.98621    0.023568  -0.20612    0.4038    -0.34364    0.27842\n",
      "  0.60846    0.30078   -0.86175   -0.87983    0.41006    0.2109\n",
      "  0.47485    0.55151    0.19373    0.15897    0.57628    0.55608\n",
      " -0.051835   0.31601    0.16209   -0.45521    0.0088562 -0.054807\n",
      "  0.17987   -0.050509   0.17955   -0.029085 ]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"\\nðŸŸ¨ GloVe vector for 'sapi':\\n\", glove['sapi'])\n",
    "except:\n",
    "    print(\"\\nðŸŸ¨ 'bakso' not found in GloVe vocab (check casing/punctuation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee4bdf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1da96927",
   "metadata": {},
   "source": [
    "any ERRRORRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "207c03a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'cinta' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m words \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maku\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcinta\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msangat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdengan\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mibuku\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m vectors \u001b[38;5;241m=\u001b[39m [w2v_model\u001b[38;5;241m.\u001b[39mwv[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m      7\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      8\u001b[0m result \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mfit_transform(vectors)\n",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m words \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maku\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcinta\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msangat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdengan\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mibuku\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m vectors \u001b[38;5;241m=\u001b[39m [w2v_model\u001b[38;5;241m.\u001b[39mwv[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m      7\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      8\u001b[0m result \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mfit_transform(vectors)\n",
      "File \u001b[1;32mc:\\Users\\backe\\miniconda3\\envs\\dl_iwan\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[1;34m(self, key_or_keys)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m \n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[1;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key_or_keys)\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[1;32mc:\\Users\\backe\\miniconda3\\envs\\dl_iwan\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \n\u001b[0;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    444\u001b[0m \n\u001b[0;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[1;32mc:\\Users\\backe\\miniconda3\\envs\\dl_iwan\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'cinta' not present\""
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = ['aku', 'cinta', 'sangat', 'dengan', 'ibuku']\n",
    "vectors = [w2v_model.wv[word] for word in words]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(vectors)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "plt.title(\"Word2Vec Embedding (PCA)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_iwan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
